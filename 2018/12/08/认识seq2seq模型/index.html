<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>认识seq2seq模型 | Lizhi&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="本文从三个方面进行展开：第一部分，介绍seq2seq在整个RNN体系中的情况。第二部分，介绍seq2seq结构的基本原理。第三部分，介绍seq2seq的缺点，和由此引出的Attention机制。  第一部分 概览RNNs： Recurrent neural network 翻译成 循环神经网络Bidirectional RNNs： 双向RNNsDeep Bidirectional RNNs: 深">
<meta name="keywords" content="理论学习">
<meta property="og:type" content="article">
<meta property="og:title" content="认识seq2seq模型">
<meta property="og:url" content="http://yoursite.com/2018/12/08/认识seq2seq模型/index.html">
<meta property="og:site_name" content="Lizhi&#39;s Blog">
<meta property="og:description" content="本文从三个方面进行展开：第一部分，介绍seq2seq在整个RNN体系中的情况。第二部分，介绍seq2seq结构的基本原理。第三部分，介绍seq2seq的缺点，和由此引出的Attention机制。  第一部分 概览RNNs： Recurrent neural network 翻译成 循环神经网络Bidirectional RNNs： 双向RNNsDeep Bidirectional RNNs: 深">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/12/08/认识seq2seq模型/pic/RNN.png">
<meta property="og:image" content="http://yoursite.com/2018/12/08/认识seq2seq模型/pic/RNN1.png">
<meta property="og:image" content="http://yoursite.com/2018/12/08/认识seq2seq模型/pic/LSTM.png">
<meta property="og:image" content="http://yoursite.com/2018/12/08/认识seq2seq模型/pic/GNU.png">
<meta property="og:image" content="http://yoursite.com/2018/12/08/认识seq2seq模型/pic/Encoder.png">
<meta property="og:image" content="http://yoursite.com/2018/12/08/认识seq2seq模型/pic/total.png">
<meta property="og:image" content="http://yoursite.com/2018/12/08/认识seq2seq模型/pic/basic.png">
<meta property="og:image" content="http://yoursite.com/2018/12/08/认识seq2seq模型/pic/seq2seq.png">
<meta property="og:image" content="http://yoursite.com/2018/12/08/认识seq2seq模型/pic/seq2seq_1.png">
<meta property="og:image" content="http://yoursite.com/2018/12/08/认识seq2seq模型/pic/seq2seq_2.png">
<meta property="og:image" content="http://yoursite.com/2018/12/08/认识seq2seq模型/pic/attention.png">
<meta property="og:image" content="http://yoursite.com/2018/12/08/认识seq2seq模型/pic/gongshi.png">
<meta property="og:updated_time" content="2018-12-09T05:12:38.393Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="认识seq2seq模型">
<meta name="twitter:description" content="本文从三个方面进行展开：第一部分，介绍seq2seq在整个RNN体系中的情况。第二部分，介绍seq2seq结构的基本原理。第三部分，介绍seq2seq的缺点，和由此引出的Attention机制。  第一部分 概览RNNs： Recurrent neural network 翻译成 循环神经网络Bidirectional RNNs： 双向RNNsDeep Bidirectional RNNs: 深">
<meta name="twitter:image" content="http://yoursite.com/2018/12/08/认识seq2seq模型/pic/RNN.png">
  
    <link rel="alternate" href="/atom.xml" title="Lizhi&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Lizhi&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Just do what you want!</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-认识seq2seq模型" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/08/认识seq2seq模型/" class="article-date">
  <time datetime="2018-12-07T16:00:00.000Z" itemprop="datePublished">2018-12-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      认识seq2seq模型
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p>本文从三个方面进行展开：<br>第一部分，介绍seq2seq在整个RNN体系中的情况。<br>第二部分，介绍seq2seq结构的基本原理。<br>第三部分，介绍seq2seq的缺点，和由此引出的Attention机制。</p>
<hr>
<h1 id="第一部分-概览"><a href="#第一部分-概览" class="headerlink" title="第一部分 概览"></a>第一部分 概览</h1><p><strong>RNNs：</strong> Recurrent neural network 翻译成 循环神经网络<br><strong>Bidirectional RNNs：</strong> 双向RNNs<br><strong>Deep Bidirectional RNNs:</strong> 深度双向RNNs<br><strong>LSTM：</strong> Long short term memory<br><strong>GRU：</strong> Gated Recurrent Unit</p>
<h2 id="解释："><a href="#解释：" class="headerlink" title="解释："></a>解释：</h2><p>  传统的神经网络很难解决基于先前事件推断后续事件的问题。<br>  RNNs是包含循环的网络，允许信息的持久化，解决了以上问题。<br><img src="pic/RNN.png" alt=""><br>  裸RNNs无法利用当前词语的下文辅助分类决策，解决方法是使用一些更复杂的RNN变种，如Bidirectional RNNs，Deep Bidirectional RNNs，使每个时刻不但接受上个时刻的特征向量，还接受来自下层的特征表示。<br><img src="pic/RNN1.png" alt=""><br>  RNNs存在的问题：在相关信息和当前预测位置之间的间隔相当长时，训练RNN变得相当困难。<br>  LSTM，作为一种特殊的RNN，可以学习长期依赖信息，避免了RNNs存在的问题。<br><img src="pic/LSTM.png" alt=""><br><img src="pic/GNU.png" alt=""><br>  GRU是LSTM的简化版。<br>  基于上面5种，建立模型，实现应用。</p>
<p><strong>Seq2seq：</strong> Sequence to sequence<br>  应用可以采用的结构有Encoder–Decoder 结构。用来处理输入的叫encoder，生成输出的叫decoder。而seq2seq就是一种输入和输出序列不等长的encoder-decoder结构，seq2seq任务的首选模型就是LSTM。这种结构可用于翻译或聊天对话场景，对输入的文本转换成另外一些列文本。如下图所示：<br><img src="pic/Encoder.png" alt=""></p>
<p>  当然，seq2seq也不是完美的，距离decoder的第一个cell越近的输入单词，对decoder的影响越大。但这并不符合常理，这时就提出了attention机制，对于输出的每一个cell，都检测输入的sequence里每个单词的重要性。</p>
<p><strong>Attention：</strong> 基于seq2seq的改进模型再进行改进的机制。<br>NMT：Neural Machine Translation 神经翻译模型<br>用一个大型神经网络建模整个翻译过程的系统。抽象的架构就是一个encoder一个decoder。<br>上面介绍的几个概念，笔者尝试性画一个自底而上的图如下所示：</p>
<p><img src="pic/total.png" alt=""></p>
<p>这些概念（或方法或模型或机制）一个个的提出，都是为了解决（优化）相应的问题，但这并不是说新的方法提出了，前者就被淘汰了，而是不同的事物的应用情景不同。不知道读者朋友们能不能体会我的感觉（我的说法也不尽全面）。</p>
<h1 id="第二部分-介绍seq2seq"><a href="#第二部分-介绍seq2seq" class="headerlink" title="第二部分 介绍seq2seq"></a>第二部分 介绍seq2seq</h1><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p><img src="pic/basic.png" alt=""></p>
<h2 id="基本模型"><a href="#基本模型" class="headerlink" title="基本模型"></a>基本模型</h2><p><img src="pic/seq2seq.png" alt=""></p>
<p>模型包括encoder和decoder两个部分。<br>将输入传到encoder部分，得到最后一个时间步长t的隐藏状态C。<br>Decoder的隐藏状态ht就由ht-1，yt-1和C三部分构成。<br>最后的输出yt从图中可知，由ht（前一时刻的隐藏层）,yt-1（前一个预测结果）,c（encoder最后一个隐藏层）得到。基本模型还有一个复杂版，如下图所示</p>
<p><img src="pic/seq2seq_1.png" alt=""></p>
<h2 id="改进模型"><a href="#改进模型" class="headerlink" title="改进模型"></a>改进模型</h2><p><img src="pic/seq2seq_2.png" alt=""></p>
<p>可以很容易的看出，改进模型与第一个模型主要的区别在于输入到输出有一条完整的流，且decoder不是每个cell都需要encoder输出隐藏状态C。</p>
<p><strong>Seq2seq模型的问题</strong><br>只能用固定维度的最后一刻的encoder隐藏层来表示源语言Y，必须将此状态一直传递下去。可以想象，当需要预测的句子相当长时，效果就会相当差。<br>于是乎引入Attention机制，改善这个问题。</p>
<h1 id="第三部分-介绍attention"><a href="#第三部分-介绍attention" class="headerlink" title="第三部分 介绍attention"></a>第三部分 介绍attention</h1><p><strong>Attention机制：</strong></p>
<p><img src="pic/attention.png" alt=""></p>
<p>相当于改变从encoder传入到decoder的隐藏状态C。 原先的状态C仅仅由最后一个时间步长t的隐藏状态C来决定。加入attention机制的seq2seq模型的状态c由所有输入向量xi共同决定。</p>
<p><img src="pic/gongshi.png" alt=""></p>
<p>即，对于decoder的时间步长i的隐藏状态si，ci等于Tx个输入向量[1,Tx]的隐藏状态与其权重αij相乘得到。权重的意义在于说明：对于当前输出的词，每一个输入的词给予的注意力是不一样的。<br>笔者仅仅将理解的部分内容转述出来，而且忽略了排版，存在漏洞在所难免，敬请指正，另附参考的博客内容，便于综合理解。</p>
<h2 id="参考内容"><a href="#参考内容" class="headerlink" title="参考内容"></a>参考内容</h2><p><a href="https://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="noopener">理解LSTM网络</a><br><a href="https://baijiahao.baidu.com/s?id=1613717006522786574&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">了解LSTM和GRU</a><br><a href="https://www.jianshu.com/p/2f48a252ad80" target="_blank" rel="noopener">CS224N学习笔记（七）—— RNN、LSTM和GRU</a><br><a href="https://blog.csdn.net/Irving_zhang/article/details/78889364" target="_blank" rel="noopener">seq2seq详解</a><br><a href="http://www.hankcs.com/nlp/cs224n-9-nmt-models-with-attention.html" target="_blank" rel="noopener">NMT与Attention</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/08/认识seq2seq模型/" data-id="cjpggclnn0001bguhl5jnh881" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/理论学习/">理论学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/12/09/学习资源/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          学习资源
        
      </div>
    </a>
  
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/plan/">plan</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/resource/">resource</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/理论学习/">理论学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/plan/" style="font-size: 10px;">plan</a> <a href="/tags/resource/" style="font-size: 10px;">resource</a> <a href="/tags/理论学习/" style="font-size: 10px;">理论学习</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/12/09/2019list/">2019 list</a>
          </li>
        
          <li>
            <a href="/2018/12/09/学习资源/">学习资源</a>
          </li>
        
          <li>
            <a href="/2018/12/08/认识seq2seq模型/">认识seq2seq模型</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Lizhi<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>